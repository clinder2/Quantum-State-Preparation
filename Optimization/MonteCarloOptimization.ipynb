{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Optimization with Monte Carlo methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "from random import randint\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['interactive'] == True\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from qiskit_aer.aerprovider import AerSimulator\n",
    "from qiskit_aer import Aer, aerprovider\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "from qiskit.circuit.library import HGate, ZGate\n",
    "from qiskit.circuit import ClassicalRegister, QuantumRegister\n",
    "from qiskit.primitives import StatevectorEstimator as Estimator\n",
    "from qiskit.primitives import StatevectorSampler as Sampler\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit.quantum_info import *"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "'''\n",
    "Reversely find a parameter.\n",
    "\n",
    "Input y is desired prepared quantum state. (L2 Normalized |y|^2 = 1)\n",
    "Input optimization_method is function used to optimize the parameters.\n",
    "Find parameters of variational quantum circuit \"RealAmplitude\" that can prepare y.\n",
    "'''\n",
    "def find_parameter(y, optimization_method: callable):\n",
    "    n = int(math.log2(len(y))) \n",
    "    ansatz = RealAmplitudes(n, reps=1) \n",
    "    parameters = np.ones(ansatz.num_parameters)  \n",
    "    simulator = AerSimulator(method='statevector') \n",
    "    meas = ClassicalRegister(1, \"meas\") \n",
    "    qreg = QuantumRegister(2 * n + 1) \n",
    "    c = QuantumCircuit(qreg, meas) \n",
    "\n",
    "    # Prepare the circuit for the swap test\n",
    "    a = [1]\n",
    "    b = [n + 1]\n",
    "    for i in range(2, n + 1):\n",
    "        a.append(i)\n",
    "        b.append(n + i)\n",
    "\n",
    "    final = c.compose(ansatz, a) \n",
    "    final.initialize(y, b) \n",
    "    final.save_statevector(label=\"ans\") \n",
    "    final.h(0) \n",
    "\n",
    "    # Perform the swap test\n",
    "    for i in range(1, n + 1):\n",
    "        final.cswap(0, i, n + i) \n",
    "    final.h(0) \n",
    "    final.measure([0], meas) \n",
    "\n",
    "    # Transpile and run the circuit\n",
    "    circ = transpile(final, simulator) \n",
    "    circfinal = circ.assign_parameters(parameters) \n",
    "    results = simulator.run(circfinal).result() \n",
    "    init_counts = results.get_counts()\n",
    "    # print(f\"Counts before optimization: {init_counts}\") \n",
    "    \n",
    "    # Visualize results of the original implementation\n",
    "    # visualize_results(init_counts, \"Before Optimization\")\n",
    "\n",
    "    # Optimization\n",
    "    Energy = E(parameters, circ, simulator) \n",
    "    anneal = optimization_method(100, parameters, circ, simulator) \n",
    "    # print(f\"Energy: {Energy}\") \n",
    "\n",
    "    # Update circuit with optimized parameters and run again\n",
    "    circfinal = circ.assign_parameters(anneal)\n",
    "    results = simulator.run(circfinal).result()\n",
    "    optimized_counts = results.get_counts() \n",
    "    \n",
    "\n",
    "    # Calculate and print results\n",
    "    b = optimized_counts.get('1', 0) # Number of times qc measured '1'\n",
    "    s = (1 - (2 / 1024) * b) # Fidelity\n",
    "    # circfinal.draw(output=\"mpl\")\n",
    "    # matplotlib.pyplot.show() \n",
    "    # print(f\"After optimization: {str(results.get_counts())}\") \n",
    "    # print(f\"Fidelity: {s}\")\n",
    "\n",
    "    # Visualize results of the Monte Carlo optimization\n",
    "    # visualize_results(optimized_counts, \"After Optimization\")\n",
    "\n",
    "    # Calculate and print partial traces\n",
    "    temp = partial_trace(results.data(0)['ans'], [0, 3, 4])\n",
    "    partial = np.diagonal(temp)\n",
    "    temp = partial_trace(results.data(0)['ans'], [0, 1, 2])\n",
    "    partial2 = np.diagonal(temp)\n",
    "    norm = np.linalg.norm(partial - partial2) \n",
    "    # print(\"Norm of the difference between partial traces:\", norm)\n",
    "\n",
    "    return parameters, s\n",
    "\n",
    "# Function to calculate cost based on parameters\n",
    "def cost_func(params, ansatz, simulator):\n",
    "    circfinal = ansatz.assign_parameters(params) \n",
    "    results = simulator.run(circfinal, shots=1024).result() \n",
    "    counts = results.get_counts() \n",
    "    b = counts.get('1', 0) \n",
    "    s = -1 * (1 - ((2 / 1024) * b)) \n",
    "    return s \n",
    "\n",
    "# Function to calculate energy based on parameters\n",
    "def E(params, ansatz, simulator):\n",
    "    circfinal = ansatz.assign_parameters(params) \n",
    "    results = simulator.run(circfinal, shots=20).result() \n",
    "    temp = partial_trace(results.data(0)['ans'], [0, 3, 4])\n",
    "    partial = np.diagonal(temp) \n",
    "    temp = partial_trace(results.data(0)['ans'], [0, 1, 2])\n",
    "    partial2 = np.diagonal(temp) \n",
    "    norm = np.linalg.norm(partial - partial2) \n",
    "    return norm \n",
    "\n",
    "# Function to visualize the results of the quantum circuit\n",
    "def visualize_results(counts, title):\n",
    "    plt.bar(counts.keys(), counts.values())\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Measurement Outcome')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Simulated Annealing optimization\n",
    "def simulated_annealing(runs, params, ansatz, simulator):\n",
    "    B = E(params, ansatz, simulator) \n",
    "    prev_E = B\n",
    "\n",
    "    # Temperature function for simulated annealing\n",
    "    def T(t):\n",
    "        c = 0.02\n",
    "        a = 0.01\n",
    "        temperature = c / (a + math.log(t)) \n",
    "        return temperature\n",
    "\n",
    "    # Main loop for simulated annealing\n",
    "    for t in range(1, runs):\n",
    "        delta = np.random.normal(0, .1, 4) \n",
    "        params_new = params + delta \n",
    "        E_new = E(params_new, ansatz, simulator) \n",
    "        delta_E = E_new - prev_E \n",
    "\n",
    "        if delta_E <= 0:\n",
    "            params = params_new\n",
    "            prev_E = E_new\n",
    "        else:\n",
    "            h = math.pow(math.e, -1 * delta_E / T(t)) \n",
    "            U = np.random.normal(.5, .5, 1) \n",
    "            if U < h:\n",
    "                params = params_new\n",
    "                prev_E = E_new\n",
    "\n",
    "    return params \n",
    "\n",
    "# Global Best Particle Swarm optimization\n",
    "def gbest_pso(runs, params, ansatz, simulator):\n",
    "    num_particles = 20  # Number of particles in the swarm\n",
    "    dimensions = len(params)  # Number of parameters to optimize\n",
    "\n",
    "    particles = np.random.rand(num_particles, dimensions) * 2 - 1  # Random positions in range [-1, 1]\n",
    "    velocities = np.zeros((num_particles, dimensions))  # Initial velocities\n",
    "    personal_best_positions = np.copy(particles)  # Initial best positions for each particle\n",
    "    personal_best_scores = [cost_func(p, ansatz, simulator) for p in particles]  # Initial best scores for each particle\n",
    "\n",
    "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]  # Best position from all particles\n",
    "\n",
    "    for j in range(runs):\n",
    "        # Update personal best and global best\n",
    "        for i in range(num_particles):\n",
    "            score = cost_func(particles[i], ansatz, simulator)  # New score\n",
    "\n",
    "            # Update personal best\n",
    "            if score < personal_best_scores[i]:\n",
    "                personal_best_scores[i] = score\n",
    "                personal_best_positions[i] = particles[i]\n",
    "\n",
    "            # Update global best\n",
    "            if score < cost_func(global_best_position, ansatz, simulator):\n",
    "                global_best_position = particles[i]\n",
    "\n",
    "        # Update velocities and positions\n",
    "        for i in range(num_particles):\n",
    "            c1 = 2.0  # Personal best weight\n",
    "            c2 = 2.0  # Global best weight\n",
    "            r1 = np.random.rand(dimensions)\n",
    "            r2 = np.random.rand(dimensions)\n",
    "\n",
    "            # Update velocity\n",
    "            velocities[i] = (0.5 * velocities[i] + \n",
    "                            c1 * r1 * (personal_best_positions[i] - particles[i]) + \n",
    "                            c2 * r2 * (global_best_position - particles[i]))\n",
    "            \n",
    "            # Limit the maximum velocity\n",
    "            max_velocity = 0.1\n",
    "            velocities[i] = np.clip(velocities[i], -max_velocity, max_velocity)\n",
    "\n",
    "            # Update position\n",
    "            particles[i] = particles[i] + velocities[i]\n",
    "\n",
    "    return global_best_position\n",
    "\n",
    "# Differential Evolution optimization\n",
    "def diff_evolution(runs, params, ansatz, simulator):\n",
    "    bounds = [(-1, 1)] * len(params)\n",
    "    dimensions = len(bounds)\n",
    "    popsize = 20\n",
    "    mut = 0.8  # Mutation factor, usually chosen from the interval [0.5, 2.0]\n",
    "    crossp = 0.7  # Crossover probability, between [0, 1]\n",
    "\n",
    "    # Initialization\n",
    "    pop = np.random.rand(popsize, dimensions)\n",
    "    min_b, max_b = np.asarray(bounds).T\n",
    "    diff = np.fabs(min_b - max_b)\n",
    "    pop_denorm = min_b + pop * diff\n",
    "    fitness = np.asarray([cost_func(ind, ansatz, simulator) for ind in pop_denorm])\n",
    "    best_idx = np.argmin(fitness)\n",
    "    best = pop_denorm[best_idx]\n",
    "\n",
    "    for i in range(runs):\n",
    "        for j in range(popsize):\n",
    "            # Mutation\n",
    "            idxs = [idx for idx in range(popsize) if idx != j]\n",
    "            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n",
    "            mutant = np.clip(a + mut * (b - c), 0, 1)\n",
    "\n",
    "            # Recombination\n",
    "            cross_points = np.random.rand(dimensions) < crossp\n",
    "            if not np.any(cross_points):\n",
    "                cross_points[np.random.randint(0, dimensions)] = True\n",
    "            trial = np.where(cross_points, mutant, pop[j])\n",
    "\n",
    "            # Replacement\n",
    "            trial_denorm = min_b + trial * diff\n",
    "            f = cost_func(trial_denorm, ansatz, simulator)\n",
    "            if f < fitness[j]:\n",
    "                fitness[j] = f\n",
    "                pop[j] = trial\n",
    "                if f < fitness[best_idx]:\n",
    "                    best_idx = j\n",
    "                    best = trial_denorm\n",
    "\n",
    "    return best\n",
    "\n",
    "# Stochastic Hill Climbing optimization\n",
    "def stochastic_hill_climbing(runs, params, ansatz, simulator):\n",
    "    max_iterations = 20\n",
    "    step_size = 0.1 # Step size for perturbation\n",
    "    threshold = 0.9\n",
    "    best_state = None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for restart in range(1, runs + 1):\n",
    "        current_state = params.copy()\n",
    "        current_score = cost_func(current_state, ansatz, simulator)\n",
    "\n",
    "        for iteration in range(1, max_iterations + 1):\n",
    "            # Apply a random perturbation to generate a neighboring state\n",
    "            neighbor = current_state + np.random.normal(0, step_size, size=current_state.shape)\n",
    "            neighbor_score = cost_func(neighbor, ansatz, simulator)\n",
    "\n",
    "            if neighbor_score < current_score:\n",
    "                current_state = neighbor\n",
    "                current_score = neighbor_score\n",
    "\n",
    "            # Update the best state and score if current is better\n",
    "            if current_score < best_score:\n",
    "                best_state = current_state\n",
    "                best_score = current_score\n",
    "\n",
    "            if current_score >= threshold:\n",
    "                break\n",
    "    return best_state"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "num_runs = 10\n",
    "sa_fidelity_values = []\n",
    "pso_fidelity_values = []\n",
    "de_fidelity_values = []\n",
    "shc_fidelity_values = []\n",
    "sa_runtimes = []\n",
    "pso_runtimes = []\n",
    "de_runtimes = []\n",
    "shc_runtimes = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    randvect = [randint(0, 100) for _ in range(4)] \n",
    "    norm = np.linalg.norm(randvect) \n",
    "    randvect = randvect / norm \n",
    "    print(f\"\\nRandom vector {i+1}: {randvect}\")\n",
    "\n",
    "    # Timing for Simulated Annealing\n",
    "    start_time = time.time()\n",
    "    original_parameters_sa, sa_fidelity = find_parameter(randvect, simulated_annealing)\n",
    "    sa_time = time.time() - start_time\n",
    "    sa_fidelity_values.append(sa_fidelity)\n",
    "    sa_runtimes.append(sa_time)\n",
    "    print(f\"Simulated Annealing: fidelity = {round(sa_fidelity, 2)}, runtime = {round(sa_time,2)} sec\")\n",
    "\n",
    "    # Timing for Particle Swarm Optimization\n",
    "    start_time = time.time()\n",
    "    original_parameters_pso, pso_fidelity = find_parameter(randvect, gbest_pso)\n",
    "    pso_time = time.time() - start_time\n",
    "    pso_fidelity_values.append(pso_fidelity)\n",
    "    pso_runtimes.append(pso_time)\n",
    "    print(f\"Particle Swarm: fidelity = {round(pso_fidelity, 2)}, runtime = {round(pso_time, 2)} sec\")\n",
    "\n",
    "    # Timing for Differential Evolution\n",
    "    start_time = time.time()\n",
    "    original_parameters_de, de_fidelity = find_parameter(randvect, diff_evolution)\n",
    "    de_time = time.time() - start_time\n",
    "    de_fidelity_values.append(de_fidelity)\n",
    "    de_runtimes.append(de_time)\n",
    "    print(f\"Differential Evolution: fidelity = {round(de_fidelity, 2)}, runtime = {round(de_time, 2)} sec\")\n",
    "\n",
    "    # Timing for Stochastic Hill Climbing\n",
    "    start_time = time.time()\n",
    "    original_parameters_shc, shc_fidelity = find_parameter(randvect, stochastic_hill_climbing)\n",
    "    shc_time = time.time() - start_time\n",
    "    shc_fidelity_values.append(shc_fidelity)\n",
    "    shc_runtimes.append(shc_time)\n",
    "    print(f\"Stochastic Hill Climbing: fidelity = {round(shc_fidelity, 2)}, runtime = {round(shc_time, 2)} sec\")\n",
    "\n",
    "# Calculate average fidelity\n",
    "sa_avg_fidelity = np.mean(sa_fidelity_values)\n",
    "pso_avg_fidelity = np.mean(pso_fidelity_values)\n",
    "de_avg_fidelity = np.mean(de_fidelity_values)\n",
    "shc_avg_fidelity = np.mean(shc_fidelity_values)\n",
    "\n",
    "# Calculate error bars for fidelity\n",
    "error_bars = [\n",
    "    [sa_avg_fidelity - np.min(sa_fidelity_values), np.max(sa_fidelity_values) - sa_avg_fidelity],\n",
    "    [pso_avg_fidelity - np.min(pso_fidelity_values), np.max(pso_fidelity_values) - pso_avg_fidelity],\n",
    "    [de_avg_fidelity - np.min(de_fidelity_values), np.max(de_fidelity_values) - de_avg_fidelity],\n",
    "    [shc_avg_fidelity - np.min(shc_fidelity_values), np.max(shc_fidelity_values) - shc_avg_fidelity]\n",
    "]\n",
    "\n",
    "# Calculate average runtimes\n",
    "sa_avg_runtime = np.mean(sa_runtimes)\n",
    "pso_avg_runtime = np.mean(pso_runtimes)\n",
    "de_avg_runtime = np.mean(de_runtimes)\n",
    "shc_avg_runtime = np.mean(shc_runtimes)\n",
    "\n",
    "# Plotting\n",
    "methods = ['Simulated Annealing', 'Particle Swarm', 'Differential Evolution', 'Stochastic Hill Climbing']\n",
    "avg_fidelity = [sa_avg_fidelity, pso_avg_fidelity, de_avg_fidelity, shc_avg_fidelity]\n",
    "avg_runtime = [sa_avg_runtime, pso_avg_runtime, de_avg_runtime, shc_avg_runtime]\n",
    "\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(methods))\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "bars1 = ax1.bar(x - bar_width/2, avg_fidelity, bar_width, label='Average Fidelity', color='tab:blue', alpha=0.7, \n",
    "                yerr=np.array(error_bars).T, capsize = 5) # Fidelity bars\n",
    "ax2 = ax1.twinx()\n",
    "bars2 = ax2.bar(x + bar_width/2, avg_runtime, bar_width, label='Average Runtime (seconds)', color='grey', alpha=0.3) # Runtime bars\n",
    "\n",
    "ax1.set_xlabel('Optimization Methods')\n",
    "ax1.set_ylabel('Average Fidelity', color='tab:blue')\n",
    "ax2.set_ylabel('Average Runtime (seconds)', color='grey')  \n",
    "ax1.set_title('Comparison of Monte Carlo Optimization Methods: Fidelity vs Runtime')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods)\n",
    "ax1.set_ylim(0, 1.2)  # Y-axis for fidelity\n",
    "ax2.set_ylim(0, max(avg_runtime) * 1.2)  # Y-axis for runtime\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum-State-Preparation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
